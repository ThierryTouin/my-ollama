services:
  ollama:
    build: 
      context: .
      dockerfile: ollama.Dockerfile    
    container_name: ollama
    hostname: ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_DEBUG=1  # Ajout de cette ligne
      - OLLAMA_GPU_LAYERS=1  # Limite l'offload Ã  XX couches sur le GPU
    healthcheck:
      test: ["CMD", "ollama", "list"]  # Check if Ollama is responding
      interval: 10s                    # Run the health check every 10 seconds
      timeout: 30s                     # Timeout for the health check is 30 seconds
      retries: 5                       # Retry 5 times before marking it as unhealthy
      start_period: 10s                # Wait 10 seconds before starting health checks
    volumes:
      - ./models:/root/.ollama/models
    networks:
    #  - genai-network
      - devnet
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    #     limits:
    #       memory: 12G  # Augmente selon ta RAM disponible

    deploy:
      resources:
        limits:
          memory: 12G



  open-webui:
    build: 
      context: .
      dockerfile: open-webui.Dockerfile    
    container_name: open-webui
    hostname: open-webui

    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./backend/data:/app/backend/data
    networks:
    #  - genai-network
      - devnet
    # depends_on:
    #   ollama-models-pull:
    #     condition: service_completed_successfully  # Wait for model pull to complete


networks:
  # genai-network:
  #   driver: bridge
  #   name: genai-network
  devnet:
    external: true
    name: devnet


    