services:
  ollama:
    image: ollama/ollama:latest
    hostname: ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "list"]  # Check if Ollama is responding
      interval: 10s                    # Run the health check every 10 seconds
      timeout: 30s                     # Timeout for the health check is 30 seconds
      retries: 5                       # Retry 5 times before marking it as unhealthy
      start_period: 10s                # Wait 10 seconds before starting health checks
    volumes:
      - ./models:/root/.ollama/models
    networks:
      - genai-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ollama-models-pull:
  #   container_name: ollama-models-pull
  #   image: curlimages/curl:latest       # Use curl to send HTTP requests
  #   depends_on:
  #     ollama:
  #       condition: service_healthy      # Wait for Ollama to be healthy
  #   command: >
  #     curl -X POST host.docker.internal:11434/api/pull -d '{"name":"mistral:latest"}'
  #     curl -X POST host.docker.internal:11434/api/pull -d '{"name":"nomic-embed-text"}'
  #   networks:
  #     - genai-network

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./backend/data:/app/backend/data
    networks:
      - genai-network
    # depends_on:
    #   ollama-models-pull:
    #     condition: service_completed_successfully  # Wait for model pull to complete


networks:
  genai-network:
    driver: bridge
    name: genai-network


    